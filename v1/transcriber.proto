syntax = "proto3";

package apis;
option go_package = ".;transcriberv1";

option java_multiple_files = true;
option java_package = "com.voicetyped.api.transcriber";


import "google/protobuf/duration.proto";
import "google/protobuf/any.proto";

message AudioFormat{

  // Defines the audio the audio format sampling rate for server to understand
  int32 sampling_rate = 1;

  // Defines the audio format sample size in bits for server to understand how to handle file
  int32 sample_size_in_bits = 2;

  // Defines the audio format channel count
  int32 channel_count = 3;

  // Defines the number of samples in the file, this is only useful when processing a file.
  int32 frame_size = 4;

  // Defines the audio format frame rate
  int32 frame_rate = 5;

  // Defines weather the bytes in the audio file are big endian
  bool is_big_endian = 6;

}

// Configuration at the start of streaming request
message TranscribeConfig {

  // Indicates the expected quality of transcription.
  // follows this law :-> you can control any two quality, speed and price
  enum Quality{
    // Here we value the speed of transcription, anything goes
    LOW = 0;
    // This is the default with a good balance on speed and accuracy of the transcription
    MEDIUM = 1;
    // The best available quality is what we care about, throw all the available resources to fullfil this goal.
    HIGH = 2;
  }

  // The encoding of the audio data sent in the request.
  //
  // All encodings support only 1 channel (mono) audio, unless the
  // `audio_channel_count` and `enable_separate_recognition_per_channel` fields
  // are set.
  //
  // For best results, the audio source should be captured and transmitted using
  // a lossless encoding (`FLAC` or `LINEAR16`). The accuracy of the speech
  // recognition can be reduced if lossy codecs are used to capture or transmit
  // audio, particularly if background noise is present. Lossy codecs include
  // `MULAW`, `AMR`, `AMR_WB`, `OGG_OPUS`, `SPEEX_WITH_HEADER_BYTE`, `MP3`,
  // and `WEBM_OPUS`.
  //
  // The `FLAC` and `WAV` audio file formats include a header that describes the
  // included audio content. You can request recognition for `WAV` files that
  // contain either `LINEAR16` or `MULAW` encoded audio.
  // If you send `FLAC` or `WAV` audio file format in
  // your request, you do not need to specify an `AudioEncoding`; the audio
  // encoding format is determined from the file header. If you specify
  // an `AudioEncoding` when you send  send `FLAC` or `WAV` audio, the
  // encoding configuration must match the encoding described in the audio
  // header; otherwise the request returns an
  // [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error
  // code.
  enum AudioEncoding {
    // Not specified.
    ENCODING_UNSPECIFIED = 0;

    // Uncompressed 16-bit signed little-endian samples (Linear PCM).
    LINEAR16 = 1;

    // `FLAC` (Free Lossless Audio
    // Codec) is the recommended encoding because it is
    // lossless--therefore recognition is not compromised--and
    // requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    // encoding supports 16-bit and 24-bit samples, however, not all fields in
    // `STREAMINFO` are supported.
    FLAC = 2;

    // 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    MULAW = 3;

    // Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
    AMR = 4;

    // Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
    AMR_WB = 5;

    // Opus encoded audio frames in Ogg container
    // ([OggOpus](https://wiki.xiph.org/OggOpus)).
    // `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
    OGG_OPUS = 6;

    // Although the use of lossy encodings is not recommended, if a very low
    // bitrate encoding is required, `OGG_OPUS` is highly preferred over
    // Speex encoding. The [Speex](https://speex.org/)  encoding supported by
    // Cloud Speech API has a header byte in each block, as in MIME type
    // `audio/x-speex-with-header-byte`.
    // It is a variant of the RTP Speex encoding defined in
    // [RFC 5574](https://tools.ietf.org/html/rfc5574).
    // The stream is a sequence of blocks, one block per RTP packet. Each block
    // starts with a byte containing the length of the block, in bytes, followed
    // by one or more frames of Speex data, padded to an integral number of
    // bytes (octets) as specified in RFC 5574. In other words, each RTP header
    // is replaced with a single byte containing the block length. Only Speex
    // wideband is supported. `sample_rate_hertz` must be 16000.
    SPEEX_WITH_HEADER_BYTE = 7;

    // Opus encoded audio frames in WebM container
    // ([OggOpus](https://wiki.xiph.org/OggOpus)). `sample_rate_hertz` must be
    // one of 8000, 12000, 16000, 24000, or 48000.
    WEBM_OPUS = 9;

    // Headerless 8-bit companded alaw samples.  used for audio compression in telecommunication systems. Like MULAW, it is commonly used in older telephony networks.
    ALAW = 10;

  }

  // Defines the quality with which our transcription is performed
  Quality quality_type = 1;

  // Encoding of audio data sent in all `RecognitionAudio` messages.
  // This field is optional for `FLAC` and `WAV` audio files and required
  // for all other audio formats.
  AudioEncoding encoding = 2;

  // Defines the audio format settings to be utilized in defining the file type
  AudioFormat audioFormat = 3;

}

// Indicates the type of speech event.
enum SpeechEventType {
  // No speech event specified.
  SPEECH_EVENT_TYPE_UNSPECIFIED = 0;

  // This event indicates that the server has detected the end of the user's
  // speech utterance and expects no additional speech. Therefore, the server
  // will not process additional audio and will close the gRPC bidirectional
  // stream. This event is only sent if there was a force cutoff due to
  // silence being detected early.
  SPEECH_ACTIVITY_END_OF_SINGLE_UTTERANCE = 1;

  // This event indicates that the server has detected the beginning of human
  // voice activity in the stream. This event can be returned multiple times
  // if speech starts and stops repeatedly throughout the stream. This event
  // is only sent if `voice_activity_events` is set to true.
  SPEECH_ACTIVITY_BEGIN = 2;

  // This event indicates that the end of human voice
  // activity in the stream. It is mostly because of a long duration of silence.
  // This event can be returned multiple times or sent multiple times by the client
  // to get a final transcription of the streamed data.
  SPEECH_ACTIVITY_END = 3;

  // This event is sent by the client to the server requesting for some provisional
  // results as the server buffers the stream. This event can be sent multiple times
  // throught the transcription process to get near realtime feedback with provisional output
  SPEECH_ACTIVITY_REQUEST_PROVISIONAL = 4;
}


// Word-specific information for recognized words.
message VoiceText {

  // The word corresponding to this set of information.
  string data = 1;

  // The confidence estimate between 0.0 and 1.0. A higher number
  // indicates an estimated greater likelihood that the recognized words are
  // correct.
  float confidence = 2;

  // A distinct label is assigned for every speaker within the audio. This field
  // specifies which one of those speakers was detected to have spoken this
  // word.
  string speaker_label = 3;

  // A label to show which language the text/data shared is in. The value can be
  // blank if the system is not able to determine the language of the text.
  string language = 4;

}

// `TranscribeResponse` is the only message returned to the client.
// A series of zero or more `TranscribeResponse`
// messages are streamed back to the client. If there is no recognizable
// audio then no messages are streamed back to the client.
//
// - In each response, only one of these fields will be set:
//     `error`,
//     `speech_event_type`, or
//     one or more (repeated) `results`.
message TranscribeResponse {

  // This repeated list contains zero or more results that
  // correspond to consecutive portions of the audio currently being processed.
  // It contains zero or one results (the interim results).
  repeated VoiceText results = 5;

  // Indicates the type of speech event.
  SpeechEventType speech_event_type = 3;

  // Time offset between the beginning of the audio and event emission.
  google.protobuf.Duration speech_event_offset = 7;

  bool provisional = 8;
}


// Request message for the
// [TranscribeRequest] message used to configure a request stream, send audio and any control signals in between.
message TranscribeRequest {
  oneof request {

    // Options to customize transcription service
    TranscribeConfig configuration = 1;

    // Indicates the type of speech event.
    SpeechEventType speech_event_type = 3;

    // Inline audio bytes to be Recognized.
    // Maximum size for this field is 15 KB per request.
    bytes audio = 5;
  }

}

message GptRequest{
  string message = 1;
  map<string, google.protobuf.Any> history = 2;
}

message GptResponse{
  string response = 1;
  map<string, google.protobuf.Any> extras = 2;
}

service TranscriberService {
  // Performs bidirectional streaming speech recognition: receive results while
  // sending audio.
  rpc Transcribe(stream TranscribeRequest) returns (stream TranscribeResponse) {}

  // Use gpt to quickly make sense of what the data is being generated quickly
  rpc Prompt(stream GptRequest) returns (GptResponse) {}
}